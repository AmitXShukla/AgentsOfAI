{"kind":"Article","sha256":"c2c4ea250dabcb56f20e59c0b86da8c559116bfcdcf6dc7e9ce24fd41e4aaa7f","slug":"gettingstarted","location":"/content/gettingstarted.md","dependencies":[],"frontmatter":{"title":"Environment Setup","content_includes_title":false,"github":"https://github.com/AmitXShukla/AgentsOfAI","edit_url":"https://github.com/AmitXShukla/AgentsOfAI/blob/main/content/gettingstarted.md","exports":[{"format":"md","filename":"gettingstarted.md","url":"/build/gettingstarted-1c81e37ac7503f3a068352a724047b06.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We have recently learned the basics of what an AI Agent is and how Multi-agent systems consist of a group of individual AI Agents working together within a single environment or across distributed environments. The primary purpose of implementing a Single Agent or a Multi-Agent system is to solve a business use case by accomplishing a specific set of tasks, thereby automating the business process.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"IU28ho1WtY"}],"key":"n7xcM7jRyS"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"However, our exploration is not complete; there is still much more to learn about AI Agents. Before we proceed to study Agent behavior in detail, letâ€™s first delve deeper into the underlying mechanisms of an AI Agent.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"cQYCiZGWsb"}],"key":"vTjtFWvVrP"},{"type":"heading","depth":2,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"new virtual environment","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"XmniEzLcSB"}],"identifier":"new-virtual-environment","label":"new virtual environment","html_id":"new-virtual-environment","implicit":true,"key":"YhaeTVY4KH"},{"type":"code","lang":"python","value":"! python --version\n# Python 3.10 or later is required.\n\n######################################################\n# Create and activate a new Python virtual environment\n# When installing AgentChat or Agent Core locally\n# we recommend using a virtual environment\n# for the installation. \n# This will ensure that the dependencies for \n# AI Agents are isolated from the rest of your system.\n######################################################\n\n! python3 -m venv .venv\n! source .venv/bin/activate\n\n## To deactivate later, run:\n! deactivate\n\n######################################################\n# install autogen in this section below\n######################################################\n# install autogen studio\n! pip install -U autogenstudio\n# install autogen chat\n! pip install -U \"autogen-agentchat\"\n# install autogen OpenAI for Model Client\n! pip install \"autogen-ext[openai]\"\n# install autogen core\n! pip install \"autogen-core\"\n\n######################################################\n# run AutoGen Studio GUI\n######################################################\n! autogenstudio ui --host <host> --port 8000","position":{"start":{"line":9,"column":1},"end":{"line":44,"column":1}},"key":"J7G6x5L5JN"},{"type":"heading","depth":2,"position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"children":[{"type":"text","value":"llama.cpp","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"key":"fXO6qLSLYF"}],"identifier":"llama-cpp","label":"llama.cpp","html_id":"llama-cpp","implicit":true,"key":"a3gPAtLgQj"},{"type":"code","lang":"python","value":"#############################################\n# install llama.cpp to run deepseek-r1 model\n#############################################\n\n! git clone https://github.com/ggerganov/llama.cpp\n! cd llama.cpp\n! make\n\n# Download the deepseek-r1 Model weights \n\n# Convert the Model (if necessary)\n# If the model weights are not already in GGML format, \n# you must convert them using the conversion tools provided in llama.cpp. \n# Assuming the model weights are in PyTorch format:\n! python3 convert-pth-to-ggml.py path/to/deepseek-r1.pth\n\n# Prepare the Model File\n# Place the .bin model file (e.g., deepseek-r1.ggml.bin) into the llama.cpp directory \n# or specify its location when running commands\n\n# run the model\n!./main -m ./deepseek-r1.ggml.bin -p \"Your prompt here\"\n\n# adjust parameters\n!./main -m ./deepseek-r1.ggml.bin -p \"Explain quantum mechanics.\" -t 8 -n 128 --temp 0.8","position":{"start":{"line":48,"column":1},"end":{"line":74,"column":1}},"key":"d7DDvBpcHT"},{"type":"heading","depth":2,"position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"children":[{"type":"text","value":"ollama","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"key":"m4UEH4h3uW"}],"identifier":"ollama","label":"ollama","html_id":"ollama","implicit":true,"key":"sEbiqql4FE"},{"type":"code","lang":"python","value":"##########################################\n# install ollama to run deepseek-r1 model\n##########################################\n\n! curl -fsSL https://ollama.com/install.sh | sh\n\n! ollama list\n! ollama pull deepseek-r1:7b\n! ollama list\n! ollama run deepseek-r1\n\n# please see, ollama by default serves at 0.0.0.0:11434\n# you can change this\n! sudo systemctl edit ollama.service\n#[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0:8000\"\n\n! sudo systemctl daemon-reload\n! sudo systemctl restart ollama.service\n\n# also, for a quick one time change you can run ollama as\n! export OLLAMA_HOST=127.0.0.1:8000\n! ollama serve","position":{"start":{"line":78,"column":1},"end":{"line":102,"column":1}},"key":"rtVhERLGCu"}],"key":"uMabtLZToU"}],"key":"c9GDr4sIti"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"More Tool Functions","url":"/moretools","group":"Tools"},"next":{"title":"Definition","url":"/agent","group":"AI Agent"}}},"domain":"http://localhost:3006"}